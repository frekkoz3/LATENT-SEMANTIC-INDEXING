{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Boolean Retrieval System with Spelling Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import total_ordering, reduce\n",
    "import csv\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@total_ordering\n",
    "class Posting:\n",
    "    \n",
    "    def __init__(self, docID):\n",
    "        self._docID = docID\n",
    "        \n",
    "    def get_from_corpus(self, corpus):\n",
    "        return corpus[self._docID]\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self._docID == other._docID\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self._docID > other._docID\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self._docID)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posting Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PostingList:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._postings = []\n",
    "    \n",
    "    @classmethod\n",
    "    def from_docID(cls, docID):\n",
    "        plist = cls()\n",
    "        plist._postings = [(Posting(docID))]\n",
    "        return plist\n",
    "    \n",
    "    @classmethod\n",
    "    def from_posting_list(cls, postingList):\n",
    "        plist = cls()\n",
    "        plist._postings = postingList\n",
    "        return plist\n",
    "    \n",
    "    def merge(self, other):\n",
    "        i = 0\n",
    "        last = self._postings[-1]\n",
    "        while (i < len(other._postings) and last == other._postings[i]):\n",
    "            i += 1\n",
    "        self._postings += other._postings[i:]\n",
    "        \n",
    "    def intersection(self, other):\n",
    "        intersection = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings) and j < len(other._postings)):\n",
    "            if (self._postings[i] == other._postings[j]):\n",
    "                intersection.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif (self._postings[i] < other._postings[j]):\n",
    "                i += 1\n",
    "            else:\n",
    "                j += 1\n",
    "        return PostingList.from_posting_list(intersection)\n",
    "    \n",
    "    def union(self, other):\n",
    "        union = []\n",
    "        i = 0\n",
    "        j = 0\n",
    "        while (i < len(self._postings) and j < len(other.postings)):\n",
    "            if (self._postings[i] == other._postings[j]):\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "                j += 1\n",
    "            elif (self._postings[i] < other._postings[j]):\n",
    "                union.append(self._postings[i])\n",
    "                i += 1\n",
    "            else:\n",
    "                union.append(other._postings[j])\n",
    "                j += 1\n",
    "        for k in range(i, len(self._postings)):\n",
    "            union.append(self._postings[k])\n",
    "        for k in range(j, len(other._postings)):\n",
    "            union.append(other._postings[k])\n",
    "        return PostingList.from_posting_list(union)\n",
    "    \n",
    "    def get_from_corpus(self, corpus):\n",
    "        return list(map(lambda x: x.get_from_corpus(corpus), self._postings))\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \", \".join(map(str, self._postings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImpossibleMergeError(Exception):\n",
    "    pass\n",
    "\n",
    "@total_ordering\n",
    "class Term:\n",
    "    \n",
    "    def __init__(self, term, docID):\n",
    "        self.term = term\n",
    "        self.posting_list = PostingList.from_docID(docID)\n",
    "        \n",
    "    def merge(self, other):\n",
    "        if (self.term == other.term):\n",
    "            self.posting_list.merge(other.posting_list)\n",
    "        else:\n",
    "            raise ImpossibleMergeError\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.term == other.term\n",
    "    \n",
    "    def __gt__(self, other):\n",
    "        return self.term > other.term\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.term + \": \" + repr(self.posting_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    no_punctuation = re.sub(r'[^\\w^\\s^-]','',text)\n",
    "    downcase = no_punctuation.lower()\n",
    "    return downcase\n",
    "\n",
    "def tokenize(movie):\n",
    "    text = normalize(movie.description)\n",
    "    return list(text.split())\n",
    "\n",
    "class InvertedIndex:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._dictionary = []\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        intermediate_dict = {}\n",
    "        for docID, document in enumerate(corpus):\n",
    "            tokens = tokenize(document)\n",
    "            for token in tokens:\n",
    "                term = Term(token, docID)\n",
    "                try:\n",
    "                    intermediate_dict[token].merge(term)\n",
    "                except KeyError:\n",
    "                    intermediate_dict[token] = term\n",
    "            if (docID % 1000 == 0):\n",
    "                print(\"ID: \" + str(docID))\n",
    "        idx = cls()\n",
    "        idx._dictionary = sorted(intermediate_dict.values())\n",
    "        return idx\n",
    "    \n",
    "    def __getitem__(self, key):\n",
    "        for term in self._dictionary:\n",
    "            if term.term == key:\n",
    "                return term.posting_list\n",
    "        raise KeyError\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"A dictionary with \" + str(len(self._dictionary)) + \" terms\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDescription:\n",
    "    \n",
    "    def __init__(self, title, description):\n",
    "        self.title = title\n",
    "        self.description = description\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.title\n",
    "    \n",
    "def read_movie_descriptions():\n",
    "    filename = 'data/plot_summaries.txt'\n",
    "    movie_names_file = 'data/movie.metadata.tsv'\n",
    "    with open(movie_names_file, 'r') as csv_file:\n",
    "        movie_names = csv.reader(csv_file, delimiter='\\t')\n",
    "        names_table = {}\n",
    "        for name in movie_names:\n",
    "            names_table[name[0]] = name[2]\n",
    "    with open(filename, 'r') as csv_file:\n",
    "        descriptions = csv.reader(csv_file, delimiter='\\t')\n",
    "        corpus = []\n",
    "        for desc in descriptions:\n",
    "            try:\n",
    "                movie = MovieDescription(names_table[desc[0]], desc[1])\n",
    "                corpus.append(movie)\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IRsystem:\n",
    "    \n",
    "    def __init__(self, corpus, index):\n",
    "        self._corpus = corpus\n",
    "        self._index = index\n",
    "        \n",
    "    @classmethod\n",
    "    def from_corpus(cls, corpus):\n",
    "        index = InvertedIndex.from_corpus(corpus)\n",
    "        return cls(corpus, index)\n",
    "    \n",
    "    def answer_query(self, words): # ['cat', 'batman']\n",
    "        norm_words = map(normalize, words)\n",
    "        postings = map(lambda w: self._index[w], norm_words)\n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)\n",
    "    \n",
    "    # New methods, this time with spelling correction!\n",
    "    def answer_query_sc(self, words):\n",
    "        norm_words = map(normalize, words)        \n",
    "        postings = []\n",
    "        \n",
    "        # Iterate over each normalized word\n",
    "        for w in norm_words:\n",
    "            try:\n",
    "                # Attempt to retrieve the postings list for the word from the index\n",
    "                res = self._index[w]\n",
    "            except KeyError:\n",
    "                # If the word is not found in the index, spelling correction is attempted\n",
    "                # Create a list of all words in the dictionary part of the index\n",
    "                dictionary = [t.term for t in self._index._dictionary]\n",
    "                # Find the nearest word from the dictionary to the misspelled word\n",
    "                sub = find_nearest(w, dictionary, keep_first=True)\n",
    "                # Notify the user of the word correction\n",
    "                print(\"{} not found. Did you mean {}?\".format(w, sub))\n",
    "                # Retrieve the postings list for the corrected word\n",
    "                res = self._index[sub]\n",
    "            # Add the postings list (original or corrected) to the list of postings\n",
    "            postings.append(res)\n",
    "        \n",
    "        plist = reduce(lambda x, y: x.intersection(y), postings)\n",
    "        return plist.get_from_corpus(self._corpus)\n",
    "    \n",
    "def query(ir, text):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_query(words)\n",
    "    for movie in answer:\n",
    "        print(movie)\n",
    "        \n",
    "def query_sc(ir, text):\n",
    "    words = text.split()\n",
    "    answer = ir.answer_query_sc(words)\n",
    "    for movie in answer:\n",
    "        print(movie)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit Distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that calculates the Levenshtein distance between two strings, u and v.\n",
    "def edit_distance(u, v):\n",
    "    # Initialize the number of rows and columns for the dynamic programming matrix.\n",
    "    # Rows represent characters of the first string, columns represent characters of the second string\n",
    "    nrows = len(u) + 1\n",
    "    ncols = len(v) + 1\n",
    "    \n",
    "    # Create a matrix with nrows rows and ncols columns initialized to zero.\n",
    "    M = [[0] * ncols for i in range(0, nrows)]\n",
    "    \n",
    "    # Initialize the first column of each row\n",
    "    # The distance from any prefix of u to an empty string v is the length of the prefix\n",
    "    for i in range(0, nrows):\n",
    "        M[i][0] = i\n",
    "    \n",
    "    # Initialize the first row of each column\n",
    "    # The distance from an empty string u to any prefix of v is the length of the prefix\n",
    "    for j in range(0, ncols):\n",
    "        M[0][j] = j\n",
    "    \n",
    "    # Compute the edit distance between every pair of prefixes of u and v\n",
    "    for i in range(1, nrows):\n",
    "        for j in range(1, ncols):\n",
    "            # Gather candidates for the edit distance:\n",
    "            # The cost of deletion from u (upward move in the matrix),\n",
    "            # and the cost of insertion to u (leftward move in the matrix).\n",
    "            candidates = [M[i-1][j] + 1, M[i][j-1] + 1]\n",
    "            \n",
    "            # Check if the characters in both strings at the current indices are equal.\n",
    "            if u[i-1] == v[j-1]:\n",
    "                # If they are equal, the previous diagonal element is a candidate.\n",
    "                candidates.append(M[i-1][j-1])\n",
    "            else:\n",
    "                # If not equal, consider the substitution cost (increment diagonal element by 1).\n",
    "                candidates.append(M[i-1][j-1] + 1)\n",
    "            \n",
    "            # The current element is the minimum of the candidates,\n",
    "            # which represents the optimal operation (insert, delete, or substitute).\n",
    "            M[i][j] = min(candidates)\n",
    "    \n",
    "    # The bottom-right element of the matrix contains the edit distance between the full strings.\n",
    "    return M[-1][-1]\n",
    "\n",
    "\n",
    "# Define a function to find the nearest word in the dictionary to the input word.\n",
    "def find_nearest(word, dictionary, keep_first=False):\n",
    "    # If keep_first is True, filter the dictionary to only include words\n",
    "    # that start with the same first letter as the input word.\n",
    "    if keep_first:\n",
    "        dictionary = list(filter(lambda w: w[0] == word[0], dictionary))\n",
    "    \n",
    "    # Calculate the edit distance between the input word and each word in the dictionary.\n",
    "    # The edit_distance function presumably calculates the Levenshtein distance.\n",
    "    # This results in a list of distances.\n",
    "    distances = map(lambda x: edit_distance(word, x), dictionary)\n",
    "    \n",
    "    # Combine each distance with the corresponding word into a list of tuples.\n",
    "    # Then find the tuple with the smallest distance.\n",
    "    # The [1] at the end extracts just the word from the tuple, ignoring the distance.\n",
    "    return min(zip(distances, dictionary))[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: 0\n",
      "ID: 1000\n",
      "ID: 2000\n",
      "ID: 3000\n",
      "ID: 4000\n",
      "ID: 5000\n",
      "ID: 6000\n",
      "ID: 7000\n",
      "ID: 8000\n",
      "ID: 9000\n",
      "ID: 10000\n",
      "ID: 11000\n",
      "ID: 12000\n",
      "ID: 13000\n",
      "ID: 14000\n",
      "ID: 15000\n",
      "ID: 16000\n",
      "ID: 17000\n",
      "ID: 18000\n",
      "ID: 19000\n",
      "ID: 20000\n",
      "ID: 21000\n",
      "ID: 22000\n",
      "ID: 23000\n",
      "ID: 24000\n",
      "ID: 25000\n",
      "ID: 26000\n",
      "ID: 27000\n",
      "ID: 28000\n",
      "ID: 29000\n",
      "ID: 30000\n",
      "ID: 31000\n",
      "ID: 32000\n",
      "ID: 33000\n",
      "ID: 34000\n",
      "ID: 35000\n",
      "ID: 36000\n",
      "ID: 37000\n",
      "ID: 38000\n",
      "ID: 39000\n",
      "ID: 40000\n",
      "ID: 41000\n",
      "ID: 42000\n"
     ]
    }
   ],
   "source": [
    "corpus = read_movie_descriptions()\n",
    "ir = IRsystem.from_corpus(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yoida not found. Did you mean yoda?\n",
      "lukke not found. Did you mean luke?\n",
      "darhth not found. Did you mean darth?\n",
      "Star Wars Episode V: The Empire Strikes Back\n",
      "Something, Something, Something Dark Side\n",
      "Return of the Ewok\n",
      "Star Wars Episode III: Revenge of the Sith\n",
      "Star Wars Episode VI: Return of the Jedi\n",
      "It's a Trap!\n"
     ]
    }
   ],
   "source": [
    "query_sc(ir, \"yoida lukke darhth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lauraa not found. Did you mean laura?\n",
      "Buried Alive\n",
      "Victim\n",
      "Huracán Ramírez\n",
      "Les Vampires\n",
      "The Good Shepherd\n",
      "Spartan\n",
      "Buried Alive II\n",
      "Sleeping with the Enemy\n",
      "The Golden Spiders: A Nero Wolfe Mystery\n",
      "Twin Peaks: Fire Walk With Me\n"
     ]
    }
   ],
   "source": [
    "query_sc(ir, \"lauraa ring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
